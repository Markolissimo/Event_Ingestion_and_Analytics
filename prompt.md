Посібник з розробки сервісу інгесту подій та аналітикиЧастина 1: Архітектурні основи — Проєктування для конфліктних навантажень1.1. Деконструкція системи: Конфлікт OLTP та OLAPПерш ніж писати код, необхідно глибоко зрозуміти суть поставленого завдання. На перший погляд, воно виглядає як стандартний CRUD-сервіс, але насправді воно приховує фундаментальний архітектурний виклик: необхідність одночасно обслуговувати два абсолютно різні типи навантажень.Інгест (Ingestion) — це Online Transaction Processing (OLTP).Цей процес характеризується великою кількістю дрібних, незалежних операцій запису. Кожен POST /events запит — це транзакція. Ключовими вимогами тут є:Висока пропускна здатність (Throughput): Система повинна бути здатною приймати тисячі подій на секунду.Низька затримка (Low Latency): API має відповідати клієнту якомога швидше, не змушуючи його чекати завершення всіх операцій збереження.Цілісність даних та ідемпотентність: Кожна подія має бути збережена рівно один раз, навіть якщо клієнт надсилає її повторно. Це вимагає надійних транзакційних механізмів.1Аналітика (Analytics) — це Online Analytical Processing (OLAP).Цей процес, навпаки, характеризується складними, важкими операціями читання, які сканують величезні обсяги даних для виконання агрегацій. Запити, такі як GET /stats/dau або GET /stats/retention, вимагають:Швидкість виконання складних запитів: Система повинна швидко обчислювати метрики (наприклад, COUNT(DISTINCT user_id), GROUP BY, JOIN) на мільйонах або навіть мільярдах записів.Оптимізація для читання: Архітектура сховища даних має бути спроєктована для швидкого сканування та агрегації, а не для точкових записів.1Спроба вирішити обидві задачі за допомогою єдиної бази даних — це архітектурний антипатерн. Традиційні реляційні бази даних (як PostgreSQL), оптимізовані для OLTP, зберігають дані порядково (row-oriented). Для розрахунку DAU такій базі доведеться зчитувати з диска цілі рядки, навіть якщо для запиту потрібні лише стовпці user_id та occurred_at, що є вкрай неефективним при великих обсягах даних.2 З іншого боку, стовпчикові (columnar) бази даних, ідеальні для OLAP, менш ефективні для частих точкових оновлень та перевірок, необхідних для забезпечення ідемпотентності під час інгесту.3 Будь-яка спроба використати один інструмент для обох завдань призведе до створення системи, яка буде посередньою в усьому і швидко стане вузьким місцем. Цей фундаментальний конфлікт вимагає більш витонченого архітектурного рішення.1.2. Рішення: Архітектура з "гарячим" та "холодним" сховищамиЩоб ефективно вирішити конфлікт між OLTP та OLAP, ми застосуємо архітектурний патерн "гарячого/холодного" зберігання даних (Hot/Cold Storage). Цей підхід, хоч і є опційним у завданні, є фундаментальним для побудови якісної та масштабованої системи.Гарячий шар (Hot Layer): Оптимізований для швидких, транзакційних записів. Його основне завдання — надійно та швидко приймати вхідні дані та забезпечувати ідемпотентність. Для цього шару ми використаємо традиційну реляційну базу даних.Холодний шар (Cold Layer): Оптимізований для аналітичних (OLAP) запитів. Дані будуть періодично переноситися з гарячого шару в холодний у форматі, придатному для швидких агрегацій. Для цього шару ми використаємо стовпчикову базу даних.Потік даних (Data Flow):Процес буде організований за принципом ETL (Extract, Transform, Load):Події надходять на API-ендпоінт.API негайно передає їх у чергу повідомлень для асинхронної обробки.Спеціальний обробник (worker) зчитує події з черги та записує їх у "гаряче" сховище (PostgreSQL), забезпечуючи ідемпотентність.Фоновий ETL-процес періодично (наприклад, раз на кілька хвилин) витягує нові дані з "гарячого" сховища, трансформує їх у потрібний формат і завантажує в "холодне" сховище (DuckDB).Аналітичні запити з API обслуговуються виключно з "холодного" сховища.Такий підхід повністю розділяє навантаження, дозволяючи кожному компоненту системи виконувати свою функцію максимально ефективно.1.3. Високорівнева схема системиНаведена нижче схема ілюструє взаємодію ключових компонентів системи:+--------+      +----------------------+      +----------------+

| Client |----->| API Service (FastAPI) |----->| Message Queue |
+--------+      +----------------------+ | (NATS) |

| ^             +----------------+
| | |
| (Analytics Queries) | (Events)
| | v
                      v | +------------------+
+---------------------+   +-------------------+ | Ingestion Worker |

| Cold Storage (OLAP) | | Hot Storage (OLTP)|<--+------------------+
| (DuckDB) | | (PostgreSQL) |
+---------------------+   +-------------------+
          ^ |

| (ETL Process) |
          +-----------------------+
Частина 2: Вибір технологічного стеку (Детальний розбір для ADR.md)Цей розділ є основою для вашого файлу ADR.md (Architecture Decision Record). Кожен вибір технології тут обґрунтовується з урахуванням вимог проєкту.2.1. API фреймворк: FastAPI як вибір професіоналівПри виборі веб-фреймворку для API в Python основними кандидатами є Flask та FastAPI.Flask — це мінімалістичний, гнучкий мікрофреймворк, який існує вже багато років.4 Він чудово підходить для швидкого прототипування або простих веб-додатків. Однак його гнучкість означає, що розробник несе повну відповідальність за вибір та інтеграцію інструментів для валідації, документації та асинхронної роботи.6FastAPI — це сучасний фреймворк, спеціально створений для розробки високопродуктивних API.5 Його архітектура надає вирішальні переваги для нашого проєкту.Ключові відмінності:Продуктивність: FastAPI побудований на ASGI (Asynchronous Server Gateway Interface), що забезпечує нативну підтримку асинхронності (async/await). Це дозволяє обробляти I/O-операції (запити до баз даних, черг повідомлень) неблокуючим чином. Бенчмарки показують, що FastAPI може обробляти 15,000–20,000 запитів на секунду, тоді як синхронний WSGI-фреймворк Flask обмежується 2,000–3,000 rps.6 Для сервісу, який інтенсивно взаємодіє з зовнішніми системами, це не просто цифри, а фундаментальна архітектурна перевага.Досвід розробника (Developer Experience): FastAPI значно спрощує розробку завдяки вбудованим інструментам:Автоматична валідація даних: Інтеграція з бібліотекою Pydantic дозволяє автоматично валідувати вхідні JSON-запити на основі Python type hints. Це усуває необхідність писати громіздкий код для перевірки даних вручну.4Автоматична генерація документації: FastAPI автоматично створює інтерактивну документацію API (Swagger UI та ReDoc) на основі коду та Pydantic-моделей. Це гарантує, що документація завжди актуальна, і значно прискорює розробку та тестування.5 У Flask для цього потрібно підключати та налаштовувати сторонні розширення.Філософія FastAPI, яка спирається на type hints та Pydantic-моделі 5, не просто валідує дані. Вона змушує розробника з самого початку чітко визначати контракти даних (data contracts) свого API. Цей більш "авторитарний" підхід 8 призводить до створення більш надійного, читабельного коду, менш схильного до помилок під час виконання. Він зміщує парадигму розробки від імперативного парсингу запитів до декларативного моделювання даних, що є значним кроком уперед у побудові систем, які легко підтримувати.Рішення: Для API-центричного сервісу, що вимагає високої продуктивності та надійної обробки даних, FastAPI є однозначно кращим вибором.2.2. Стратегія баз даних: Історія про два сховищаЯк було зазначено, ми використовуємо архітектуру з "гарячим" та "холодним" шарами. Вибір конкретних баз даних для кожного шару є критично важливим.Гаряче сховище (для інгесту): PostgreSQLОбґрунтування: Процес інгесту вимагає транзакційної цілісності (ACID-сумісності) для надійної обробки перевірок на ідемпотентність та записів. PostgreSQL — це перевірена часом, надзвичайно надійна реляційна база даних, яка ідеально підходить для цього OLTP-навантаження.1 Її зрілість, стабільність та потужні можливості роблять її професійним стандартом для основного сховища даних сервісу.Холодне сховище (для аналітики): DuckDBОбґрунтування: Аналітичні ендпоінти вимагають швидких агрегацій на великих наборах даних. Це класичне OLAP-навантаження, де стовпчикові бази даних демонструють найкращі результати.1Порівняння DuckDB та ClickHouse: Це порівняння є ключовим для виконання вимоги "використати новий інструмент".ClickHouse — це розподілений, серверний OLAP-монстр, створений для петабайтних масштабів та розгортання на кластерах з багатьох вузлів.11 Він забезпечує неймовірну продуктивність для аналітики в реальному часі на величезних даних. Однак для нашого проєкту він є архітектурним надлишком (overkill) і вносить операційну складність (необхідність керувати окремим кластером серверів).13DuckDB — це вбудована (in-process) OLAP-база даних, яку часто називають "SQLite для аналітики".12 Вона не має зовнішніх залежностей, працює в межах процесу Python-додатку, що усуває мережеві затримки та спрощує архітектуру.12Вибір між DuckDB та ClickHouse — це не лише питання продуктивності, а й архітектурної відповідності. Дослідження показують, що ClickHouse домінує на масивних, розподілених навантаженнях 11, але DuckDB часто є швидшим для аналітики на рівні додатку, що працює на одному вузлі.12 Наш проєкт буде працювати в середовищі docker-compose на одній машині. Використання ClickHouse тут було б схоже на використання вантажного корабля для доставки піци. Вбудована природа DuckDB ідеально відповідає обмеженням проєкту, спрощує архітектуру, зменшує навантаження на ресурси та забезпечує виняткову продуктивність для заданого масштабу, прямо відповідаючи цілям проєкту та вимогам до навчання.Рішення: PostgreSQL для "гарячого" сховища, DuckDB для "холодного".Матриця рішень щодо технологій баз данихТехнологіяОсновне навантаженняАрхітектурна модельМасштабованістьКлючові перевагиКлючові недолікиВідповідність проєктуPostgreSQLOLTP (інгест, ідемпотентність)Реляційна, порядковаВертикальнаACID-сумісність, надійність, зрілістьПовільні аналітичні запити на великих данихВисока (для гарячого шару)ClickHouseOLAP (аналітика в реальному часі)Стовпчикова, розподіленаГоризонтальнаНадзвичайна швидкість на петабайтах, висока пропускна здатністьСкладність налаштування та підтримки, надлишковість для малих масштабівНизька (надлишковий)DuckDBOLAP (вбудована аналітика)Стовпчикова, вбудованаОдно-вузловаНульова конфігурація, відсутність сервера, висока швидкість для локальних данихНе призначений для розподілених навантаженьІдеальна (для холодного шару)2.3. Конвеєр інгесту: NATS з JetStreamРоз'єднання API та запису в базу даних за допомогою черги повідомлень є критично важливим для низької затримки, високої пропускної здатності та відмовостійкості системи.Порівняння NATS, RabbitMQ та Redis:RabbitMQ — це зрілий, повнофункціональний брокер повідомлень, але він може бути складним у налаштуванні та підтримці.17 Він підтримує складні механізми маршрутизації, які нам не потрібні.Redis Pub/Sub — дуже швидкий, але за замовчуванням не надає надійних гарантій доставки та збереження даних (persistence).19NATS — це сучасна, легка та надзвичайно продуктивна система обміну повідомленнями.17 З додаванням компонента JetStream вона отримує необхідні для надійного конвеєра гарантії персистентності та доставки "щонайменше один раз" (at-least-once).17Вибір черги повідомлень часто є компромісом між продуктивністю та надійністю. NATS з JetStream руйнує цю дихотомію. Він забезпечує продуктивність, яка значно перевищує RabbitMQ (сотні тисяч повідомлень на секунду проти десятків тисяч) 17, водночас пропонуючи критично важливі функції для нашого проєкту (персистентність, гарантії доставки). Його сучасний, хмарно-орієнтований дизайн та простота роблять його чудовим інструментом для вивчення та кращим архітектурним вибором, ніж більш важкий RabbitMQ для цього конкретного випадку.Рішення: NATS з JetStream є ідеальним вибором, що відповідає вимозі до навчання, забезпечуючи при цьому вищу продуктивність та простішу операційну модель.Частина 3: Детальний розбір реалізації3.1. Структурування проєкту для успіхуЧиста та логічна структура проєкту є запорукою його успішної розробки та подальшої підтримки. Рекомендується наступна організація файлів:/event_service
├── app/
│   ├── api/          # Роутери FastAPI та залежності
│   │   ├── endpoints/
│   │   └── deps.py
│   ├── core/         # Основна логіка, конфігурація, rate limiting
│   │   ├── config.py
│   │   └── security.py
│   ├── crud/         # Логіка взаємодії з базами даних (CRUD)
│   │   ├── crud_event_pg.py
│   │   └── crud_analytics_duckdb.py
│   ├── models/       # Моделі Pydantic та SQLAlchemy
│   │   ├── event.py
│   │   └── stats.py
│   ├── schemas/      # Pydantic схеми для API
│   │   └── event.py
│   ├── worker/       # Споживач NATS та логіка інгесту
│   │   └── consumer.py
│   └── main.py       # Точка входу додатку FastAPI
├── cli/
│   └── import_events.py # Скрипт для масового імпорту
├── tests/
│   ├── unit/
│   └── integration/
├── docker-compose.yml
├── Dockerfile
├── ADR.md
├── LEARNED.md
└── README.md
3.2. Створення API інгесту (POST /events)Контракти даних з PydanticОсновою надійного API є чітко визначені контракти даних. Ми визначимо Pydantic-модель, яка буде єдиним джерелом правди для структури події.Python# app/schemas/event.py
from datetime import datetime
from uuid import UUID
from pydantic import BaseModel, Field

class Event(BaseModel):
    event_id: UUID
    occurred_at: datetime
    user_id: str  # Може бути UUID або інший ідентифікатор
    event_type: str = Field(..., min_length=1, max_length=100)
    properties: dict

    class Config:
        from_attributes = True
FastAPI буде використовувати цю модель для автоматичної валідації вхідного JSON-масиву.9 Pydantic автоматично перетворить рядок у форматі ISO 8601 на об'єкт datetime і надасть детальні помилки валідації у разі невідповідності даних.22Асинхронний потікЕндпоінт API не буде записувати дані в базу даних напряму. Це забезпечить низьку затримку та високу відмовостійкість.Валідація: Ендпоінт прийматиме list[Event] і FastAPI автоматично провалідує кожен об'єкт у масиві.Публікація в чергу: Кожна валідна подія буде серіалізована в JSON і опублікована як повідомлення в тему (subject) NATS.Відповідь: API негайно поверне клієнту відповідь 202 Accepted, сигналізуючи, що запит прийнято до обробки, але ще не завершено.Python# app/api/endpoints/events.py
from fastapi import APIRouter, status, Depends
from nats.aio.client import Client as NATSClient
from app.schemas.event import Event
from app.api.deps import get_nats_client

router = APIRouter()

@router.post("/events", status_code=status.HTTP_202_ACCEPTED)
async def ingest_events(
    events: list[Event],
    nc: NATSClient = Depends(get_nats_client)
):
    # NATS_SUBJECT - назва теми, наприклад, "events.ingest"
    for event in events:
        await nc.publish("events.ingest", event.model_dump_json().encode())
    return {"message": f"{len(events)} events accepted for processing."}
Обробник інгесту (Worker)Це буде окремий Python-процес, який виконує "брудну" роботу:Підключення та підписка: Worker підключається до NATS і підписується на тему events.ingest.Споживання повідомлень: Він споживає повідомлення (можливо, пакетами для ефективності).Ідемпотентний запис: Для кожної події виконується операція "upsert" у "гарячому" сховищі (PostgreSQL). Використання INSERT... ON CONFLICT DO NOTHING на унікальному індексі по event_id є найпростішим та найефективнішим способом забезпечити ідемпотентність.Підтвердження: Після успішного запису в базу даних, worker надсилає підтвердження (acknowledgement) NATS, щоб повідомлення було видалено з черги.Обробка помилок: Ми налаштуємо NATS JetStream на повторні спроби (retries) у разі невдалого запису та на переміщення повідомлень у чергу "мертвих листів" (dead-letter queue) після кількох невдалих спроб.3.3. Створення аналітичного API (GET /stats)Цей розділ зосереджений на створенні ефективних SQL-запитів до нашого "холодного" сховища DuckDB. DuckDB може виконувати запити безпосередньо до файлів Parquet або до власної бази даних, що робить ETL-процес дуже простим.DAU (/stats/dau)Запит для отримання кількості унікальних активних користувачів по днях є досить простим:SQLSELECT
    DATE_TRUNC('day', occurred_at) AS day,
    COUNT(DISTINCT user_id) AS dau
FROM events
WHERE occurred_at BETWEEN? AND?
GROUP BY day
ORDER BY day;
Топ подій (/stats/top-events)Запит для отримання найпопулярніших типів подій:SQLSELECT
    event_type,
    COUNT(*) AS event_count
FROM events
WHERE occurred_at BETWEEN? AND?
GROUP BY event_type
ORDER BY event_count DESC
LIMIT?;
Когортний аналіз (/stats/retention): Майстер-клас з SQLЦе найскладніший аналітичний запит. Ми побудуємо його крок за кроком, використовуючи загальні табличні вирази (Common Table Expressions, CTEs), оскільки це робить логіку запиту значно зрозумілішою та легшою для підтримки.23 Ми оберемо тижневі вікна для аналізу.CTE user_cohorts: Визначаємо когорту для кожного користувача. Когорта — це тиждень, коли користувач вперше проявив активність.SQLWITH user_cohorts AS (
    SELECT
        user_id,
        DATE_TRUNC('week', MIN(occurred_at)) AS cohort_week
    FROM events
    GROUP BY user_id
)
CTE user_activities: Приєднуємо таблицю подій до когорт, щоб визначити, в який тиждень відносно дати реєстрації відбулася кожна наступна активність. Ми розраховуємо week_number — різницю в тижнях між датою події та датою когорти.26SQL, user_activities AS (
    SELECT
        e.user_id,
        (DATE_TRUNC('week', e.occurred_at)::DATE - uc.cohort_week::DATE) / 7 AS week_number,
        uc.cohort_week
    FROM events e
    JOIN user_cohorts uc ON e.user_id = uc.user_id
)
CTE cohort_size: Розраховуємо початковий розмір кожної когорти — кількість унікальних користувачів, які з'явилися в певний тиждень.SQL, cohort_size AS (
    SELECT
        cohort_week,
        COUNT(DISTINCT user_id) AS total_users
    FROM user_cohorts
    GROUP BY cohort_week
)
CTE retention_counts: Рахуємо кількість унікальних користувачів з кожної когорти, які були активні в кожен наступний тиждень (week_number).SQL, retention_counts AS (
    SELECT
        cohort_week,
        week_number,
        COUNT(DISTINCT user_id) AS retained_users
    FROM user_activities
    GROUP BY cohort_week, week_number
)
Фінальний SELECT: Об'єднуємо результати для розрахунку відсотка утримання для кожної когорти та кожного тижня.SQLSELECT
    rc.cohort_week,
    cs.total_users,
    rc.week_number,
    rc.retained_users,
    (rc.retained_users::FLOAT / cs.total_users) * 100 AS retention_percentage
FROM retention_counts rc
JOIN cohort_size cs ON rc.cohort_week = cs.cohort_week
WHERE rc.cohort_week >=? -- start_date
ORDER BY rc.cohort_week, rc.week_number;
Цей запит надає повну картину утримання користувачів, дозволяючи аналізувати, як змінюється поведінка користувачів з різних когорт з часом.3.4. Масовий імпортер даних (CLI)Для початкового завантаження історичних даних ми створимо професійний CLI-скрипт за допомогою бібліотеки Typer або Click.Читання по частинах (Chunking): Скрипт буде читати CSV-файл частинами (наприклад, по 10,000 рядків за раз), використовуючи pandas.read_csv з параметром chunksize. Це дозволить уникнути завантаження всього файлу в пам'ять, що критично для великих датасетів.Масова вставка: Для кожної частини даних скрипт буде формувати ефективні INSERT запити з виразом ON CONFLICT (event_id) DO NOTHING для масової вставки в PostgreSQL, що забезпечить високу швидкість та ідемпотентність.Частина 4: Забезпечення готовності до продакшену4.1. Контейнеризація за допомогою docker-compose.ymlДля простоти розгортання та відтворюваності середовища ми визначимо всі компоненти системи як сервіси в docker-compose.yml:api: Додаток FastAPI.worker: Обробник інгесту NATS.postgres: База даних "гарячого" шару.nats: Брокер повідомлень NATS з увімкненим JetStream.etl: Окремий сервіс, що періодично запускає ETL-скрипт для перенесення даних з Postgres в DuckDB.Окремий сервіс для DuckDB не потрібен, оскільки він працює вбудовано в сервіс api. Це ще раз підкреслює перевагу нашого вибору.4.2. Прагматична стратегія тестуванняМодульні тести (Unit Tests) з pytest: Ми напишемо модульні тести для ізольованої бізнес-логіки. Наприклад, для перевірки логіки ідемпотентності в crud шарі, де підключення до бази даних буде імітовано (mocked).Інтеграційний тест: Ми напишемо наскрізний тест, який перевіряє весь "щасливий шлях" системи:За допомогою тестового клієнта FastAPI надсилаємо POST запит з тестовою подією на /events.Чекаємо короткий проміжок часу, щоб worker встиг обробити повідомлення.Запускаємо ETL-процес.Надсилаємо GET запит на аналітичний ендпоінт (наприклад, /stats/dau) і перевіряємо, що дані з тестової події враховані в метриках.Цей тест перевіряє коректну взаємодію всіх компонентів: API -> NATS -> Worker -> PostgreSQL -> ETL -> DuckDB -> API.4.3. Спостережуваність та продуктивністьСтруктуровані логи: Ми використаємо бібліотеку structlog для виводу логів у форматі JSON з усіх сервісів. Це робить їх легко парсованими, що спрощує пошук та аналіз у системах збору логів (наприклад, ELK Stack або Loki).Метрики: Ми додамо простий middleware до FastAPI для відстеження базових метрик: кількість запитів (request count), частота помилок (error rate) та затримка відповіді (response latency).Бенчмаркінг:Методологія: Створимо скрипт (наприклад, на locust або простому asyncio), який генерує та надсилає 100,000 подій на ендпоінт /events. Заміряємо загальний час інгесту. Після цього зробимо запит до /stats/dau і заміряємо час відповіді.Аналіз вузьких місць: Результати, найімовірніше, покажуть, що вузьким місцем буде не сам сервіс FastAPI, а єдиний worker, який записує дані в PostgreSQL. API залишиться швидким та чутливим завдяки асинхронній передачі даних у NATS.Пропоновані виправлення: Рішенням проблеми вузького місця є горизонтальне масштабування worker-ів. Оскільки NATS підтримує конкуруючих споживачів у черзі, ми можемо просто запустити більше екземплярів worker-ів, які будуть паралельно обробляти повідомлення, значно збільшуючи пропускну здатність інгесту.4.4. Зміцнення сервісуВалідація: Pydantic забезпечує основний шар валідації вхідних даних.Обробка помилок: Ми реалізуємо власні обробники винятків у FastAPI для повернення чітких, структурованих JSON-відповідей з відповідними HTTP-статус-кодами (наприклад, 400, 422, 500).Обмеження частоти запитів (Rate Limiting): Ми реалізуємо простий in-memory rate limiter на основі алгоритму "token bucket" як залежність FastAPI для захисту сервісу від зловживань.Частина 5: Синтез та рефлексія5.1. Документування отриманих знань (LEARNED.md)Цей документ має бути розповіддю про ваш досвід роботи з новими інструментами.Про DuckDB: "Я дізнався, що DuckDB — це надзвичайно потужна вбудована OLAP-база даних. Її ключова перевага — нульова конфігурація та відсутність окремого серверного процесу, що робить її ідеальною для вбудовування аналітики безпосередньо в додатки. Продуктивність для аналітичних запитів на одному вузлі була винятковою, часто перевершуючи серверні рішення для такого масштабу. Я налаштував її, просто встановивши Python-пакет і вказавши шлях до локального файлу; дані записувалися з Pandas DataFrame, що робить ETL-процес тривіальним."Про NATS: "Я дізнався, що NATS — це високопродуктивна система обміну повідомленнями, яка разом з компонентом JetStream є переконливою альтернативою RabbitMQ. Її головна сила — простота та швидкість. Я налаштував довговічний потік (durable stream) та споживача на основі pull-моделі в JetStream, що забезпечило гарантії доставки "щонайменше один раз" та персистентність повідомлень. Сервер, написаний на Go, був легким і простим для запуску в Docker. Головний висновок — його придатність для сучасних, хмарно-орієнтованих систем, де продуктивність та операційна простота є першочерговими."5.2. Фінальний огляд архітектури (Резюме для README.md)На завершення, в README.md необхідно підсумувати прийняті архітектурні рішення. Слід ще раз наголосити, чому була обрана архітектура "гарячий/холодний шар" з чергою повідомлень і як вона ефективно вирішує основний конфлікт OLTP/OLAP. Також варто коротко описати можливі шляхи подальшого розвитку системи, такі як масштабування worker-ів, додавання автентифікації та перехід на керовані хмарні сервіси баз даних або черг повідомлень.